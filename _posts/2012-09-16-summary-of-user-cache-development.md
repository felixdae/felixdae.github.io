---
title: user cache开发小结
---

{{ page.title }}
===============

最近有一个大量访问用户资料的应用，场景是要随机的访问用户数据，一个用户的资料一天之内被访问的频度并不大，但是每天要访问的用户资料的绝对总量非常大。

在这种情况下，数据库的压力会增大，因此我们决定做缓存，但由于各个用户数据本身的热度并不高，即访问一次之后再次被访问的次数并不高，相反正是需要缓存来应付第一次被访问的情况以分摊数据库压力，因此使用传统的第一次访问的时候增加相应其缓存的方案在这里并不合适，考虑到需要访问的用户数据的大多数内容是不变的，我们决定采取遍历整个用户表，造所有用户数据的全量cache的方案。但是这里有个问题，虽然要访问的用户资料大多数内容是不会更改的，但是在这个应用中我们确实也需要跟踪几个关键字段的更新情况。对于这一问题，一种自然的解决方案是在所有操作这几个字段的接口处触发相应用户资料缓存的更新，这一方案的弊端是由于历史原因，操作这一字段的接口数量很多而且十分分散，要加相关缓存更新代码工作量会比较大，而且还都是重复工作；更重要的是这些字段牵涉关键业务，我们不想在操作这些字段的接口处加上过多的非核心代码，以避免影响这些接口的效率和功能。幸好的是，对于所有这些可变字段的操作还有一个辅助的流水表，记录了这些字段的变化过程，因此我们改变方案，希望通过监控流水表的变化情况来间接的监控用户资料变化，一旦发现出现某用户有新的流水记录，就去更新这个用户资料的缓存，这样就可以达到同步可变字段的目的。

在具体实现上有遇到几个问题：

1，既然要造全量cache，就必然要遍历全表。由于数据量较大，无条件select加limit分页的方式无疑是个慢速sql，会对数据库造成极大负担。事实上我们确实出现过因这种拉取数据方式造成数据库崩溃的事故。因为用户表的主键是整型的，因此我们决定采取以主键范围作为where条件的方案，具体来说就是条件设置为i<=pk and pk<i+offset，并在每次查询之后将新的i更新为i+offset继续查询，直至i超过之前保存的最大主键值，这里从查询最大主键值到遍历整表并没有锁表，最新注册的用户是有可能不会被扫到，但因为用户注册的入口是唯一的，因此我们在此触发新注册用户的缓存新增就能解决这一问题。另一方面从我们的应用场景来说我们是为了分摊数据库的压力，其实也并不需要100%的全量cache，比例达到一定的数值就ok了。

2，可变字段更新的实现。上面讲到用监控流水表的形式来监控这些字段的变化，具体来讲就是首先在造全量cache的时候我们就记住此时流水表的最大流水号作为初值，流水号是自增的整型主键，此后独立的缓存更新进程每次循环过程中都会去查询当前最大流水，然后获取当前流水和上次保存的流水之间有哪些用户有过更新，再去更新这些用户的缓存，并更新最新的最大流水号。为防止程序异常崩溃的情形下，最大流水号丢失的情况，缓存更新程序会每隔一段时间把内存中的最大流水号同步到磁盘，在我们的实现中即是同步到嵌入式数据库sqlite之中。

3，程序的结构。从之前的描述中，我们知道这个系统至少需要两种程序：造全局cache的程序和更新cache的程序。造全局cache的程序因为是一次性运行，为方便调试和跟踪，可以写成前台程序。考虑到用户数据量极大，我们必须采用多进程或者多线程的方式，这样的实现方式也能更好的适应已横向分表了的用户表，具体来说就是一个进程负责若干个分表的数据遍历和cache产生。我们把造全局cache失败重试的最小粒度定在分表级别，原因也是上面所讲的我们并不追求100%的全量cache，有个别用户造cache失败我们并不需在意，如果某个表有大量用户新增cache失败，这个时候我们可以选择同时也可以做到对该分表单独重做cache。而更新程序则应该是一个长期运行的后台运行的程序，这个时候我们应该将其daemon化，同样由于分表和大数据量的特征，也应该是多进程或者多线程配合的形式，在具体实现中我们采取了一个父进程连同若干子进程的结构，这样不但可以便于各进程间共享资源，同时也能较好支持父进程对各子进程的监控。

